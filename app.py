import sys
try:
    import pysqlite3 as sqlite3
    sys.modules['sqlite3'] = sqlite3
except ImportError:
    import sqlite3
import time  # Import the time module


from flask import Flask, request, jsonify
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders.pdf import PyMuPDFLoader
from langchain.document_loaders.xml import UnstructuredXMLLoader
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.llms import HuggingFaceHub
from langchain.chains import LLMChain
import os

app = Flask(__name__)


loaders = {
    '.pdf': PyMuPDFLoader,
    '.xml': UnstructuredXMLLoader,
    '.csv': CSVLoader,
}
def create_directory_loader(file_type, directory_path):
    return DirectoryLoader(
        path=directory_path,
        glob=f"**/*{file_type}",
        loader_cls=loaders[file_type],
    )
# Create DirectoryLoader instances for each file type
pdf_loader = create_directory_loader('.pdf', './content/data')
# xml_loader = create_directory_loader('.xml', '/path/to/your/directory')
# csv_loader = create_directory_loader('.csv', '/path/to/your/directory')
# Load the files
pages = pdf_loader.load_and_split()


# Load an embedding model from hugging face.
model_name = "BAAI/bge-large-en-v1.5"
model_kwargs = {'device': 'cuda'}
encode_kwargs = {'normalize_embeddings': True}

embed_model = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

text_splitter = SemanticChunker(embed_model)

# split the pages using Semantic Chunker.
documents = text_splitter.split_documents(pages)

# embed and insert all chunks of the documents into the vector database
vector_db = Chroma.from_documents(
    documents, 
    embed_model, # model to use for embedding the document chunks before storing.
    persist_directory='vector_db', # persist the database in memory.
    collection_name='ai_career' # name of the collection to store the chunks in.
)

def join_retrieved_docs(docs):
    return "\n\n".join([doc.page_content for doc in docs])

# Template so we can attach our context and query as prompt to the LLM on the fly.
template = """Answer the question using vital information from the following context, 
if the context is relevant, if context given is not relevant, 
you should reply >>> 'Sorry, But the context provided doesn't contain enough or relevant
information to answer your question'

>>> 'Context : {context}'

>>> 'Question: {question}'
"""
prompt = ChatPromptTemplate.from_template(template)
repo_id = "google/flan-t5-xxl" 

llm = HuggingFaceHub(
    repo_id=repo_id,
    model_kwargs={
        "temperature": 0.1 
    }
)

def query_llm_with_context_method(question, llm, prompt_template):
    start_time = time.time()
    context = vector_db.similarity_search(question, k=5)
    search_time = time.time() - start_time

    llm_chain_init_start = time.time() 
    llm_chain = LLMChain(
        prompt=prompt_template, 
        llm=llm
    )
    llm_chain_init_time = time.time() - llm_chain_init_start 

    llm_invoke_start = time.time() 
    llm_response = llm_chain.invoke(
        input = {
            'context' : join_retrieved_docs(context),
            'question' : question
                })
    llm_invoke_time = time.time() - llm_invoke_start
    # Return the text of the response generated by the language model
    print(f"Similarity search time: {search_time:.2f} seconds")
    print(f"LLMChain initialization time: {llm_chain_init_time:.2f} seconds")
    print(f"LLM invocation time: {llm_invoke_time:.2f} seconds")
    return llm_response['text']


@app.route('/query', methods=['POST'])
def query_llm_with_context():
    # Parse the request data to get the question
    data = request.get_json()
    question = data.get('question')
    print("-----------USER question is", question)
    if not question:
        return jsonify({"error": "No question provided"}), 400
    
    answer = query_llm_with_context_method(question=question, llm=llm, prompt_template=prompt)
    return jsonify({"response": answer})

if __name__ == "__main__":
    app.run(debug=True)